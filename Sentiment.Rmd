---
title: "Sentiment and Model"
author: "Bugra Duman"
date: "2/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## SVM model and Sentiment Analysis

```{r include=FALSE}
library(pander)
library(tidyr)
library(tidytext)
library(tm)
library(stringi)
```

Creating Dataframe from raw dataset by selecting only related columns.

```{r Table}
dt <- data.frame(r$airline,r$overall,r$cabin,r$traveller_type,r$customer_review,r$recommended_yes)
pandoc.table(dt[2:4,1:2], justify = c('left', 'left', 'left'))
```

## Cleaning

In order to have better understanding for customer review we need to clean by removing punctuation, numbers and stop words like "me", "was", "were","your".

```{r Corpus and Cleaning, echo=FALSE}
#Setting up source and corpus

corpus <- VCorpus(VectorSource(r$customer_review))
corpus[[123]]$content

# Cleaning
corpus<- tm_map(corpus,stripWhitespace)

corpus <- tm_map(corpus,removePunctuation)

corpus <- tm_map(corpus,content_transformer(tolower))

corpus<- tm_map(corpus,removeWords,stopwords("english"))

#since it is a flight review it has lots of time info lets remove
corpus <- tm_map(corpus,removeNumbers)

#last Whitespaceremove

corpus<- tm_map(corpus,stripWhitespace)


```

Since this is a scraped data from [skytrax](https://www.airlinequality.com/airline-reviews/) it has special characters and we need to update the stop words with our custom stop words for this dataset which you can see below.

```{r CustomClean}
#update to space function
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))



corpus <- tm_map(corpus,toSpace, c("âœ… | Not Verified | âœ… verified | âœ… trip verified | Verified | trip verified | verified review | flight | airline | airlines"))


custom_stop_words <-tribble(
  ~word, ~lexicon,
  "âœ… ", "CUSTOM",
  "Not Verified", "CUSTOM",
  "âœ… verified","CUSTOM",
  "âœ… trip verified ", "CUSTOM",
  "Verified","CUSTOM",
  "trip verified","CUSTOM",
  "verified review", "CUSTOM",
  "flight","CUSTOM",
  "airlines","CUSTOM",
  "airline", "CUSTOM",
  "review", "CUSTOM",
  "verified","CUSTOM",
  "âœ","CUSTOM",
  "flights","CUSTOM",
  "plane", "CUSTOM"
  
)

stop_words2 <- stop_words %>%
  bind_rows(custom_stop_words)



corpus <- tm_map(corpus,removeWords,c(stop_words2$word))


corpus <- tm_map(corpus,stripWhitespace)

#checking if everything is correct
corpus[[123]]$content
```

Let\`s check if our cleaning worked.

```{r DTM}

dtm_airline <- DocumentTermMatrix(corpus)

inspect(dtm_airline)

freq_terms_airline <- findFreqTerms(dtm_airline,5)

head(freq_terms_airline,20)


```

As we can see our most frequent terms are not makes any sense we will continue to clean

```{r Cleaning}
corpus<- tm_map(corpus,toSpace,"â.*")


corpus <- tm_map(corpus,stripWhitespace)
#corpus <- tm_map(corpus,stemDocument)
corpus[[123]]$content
dtm2 <- DocumentTermMatrix(corpus)

inspect(dtm2)

sparse <- removeSparseTerms(dtm2,0.995)
dtsparse <- as.data.frame(as.matrix(sparse))
colnames(dtsparse) <- make.names(colnames(dtsparse))

dtsparse$recommended_yes <- dt$r.recommended_yes


```

Better results with only words.

### Modeling

Below I have created test train split with %70 ratio.

```{r}
{r Modelling}

library(caTools)
library(e1071)
set.seed(1234)

split <- sample.split(dtsparse$recommended_yes, SplitRatio =  .7)

trainSparse <- subset(dtsparse,split== TRUE)
testSparse <- subset(dtsparse,split == FALSE)


trainSparses <- subset(trainSparse[1:20000,])


trainSparses2 <- subset(trainSparse[20000:46163,])


frqtab <- function(x, caption) {
  round(100*prop.table(table(x)), 1)
}

ft_orig <- frqtab(dtsparse$recommended_yes)
ft_train <- frqtab(trainSparses$recommended_yes)
ft_test <- frqtab(testSparse$recommended_yes)
ft_df <- as.data.frame(cbind(ft_orig, ft_train, ft_test))
colnames(ft_df) <- c("Original", "Training set", "Test set")
pander(head(ft_df), caption="Yes/No in Test and Training Sets")


```

As we can see from the table we have balanced train and test dataset.

```{trainSparse$recommended_yes <- as.factor(trainSparse$recommended_yes)}
testSparse$recommended_yes <- as.factor(testSparse$recommended_yes)
svm_model <- svm(recommended_yes ~ . ,data = trainSparses, scale = FALSE)
svm_model2 <- svm(recommended_yes ~ ., data = trainSparses2,scale = FALSE )

summary(svm_model)

predic_svm <- predict(svm_model,newdata = testSparse)
predic_svm2 <- predict(svm_model2,newdata = testSparse)

review_svm_model.perf2 <- table(na.omit(testSparse$recommended_yes), predic_svm2, dnn= c("Actual", "Predicted"))
review_svm_model.perf <- (table(na.omit(testSparse$recommended_yes), predic_svm, dnn=c("Actual", "Predicted")))
review_svm_model.perf
review_svm_model.perf2

conf <- confusionMatrix(predic_svm2, testSparse$recommended_yes, positive = '1')



```

Our model predicted 85% accuracy if a person will recommend this airline or not according to review.

#### Sentiment Analysis

We are going to use our first dataset so we have to clean again the numbers and stop words. After cleaning we can see the most common words.

```{r Token}


dt$r.customer_review <- gsub("[[:digit:]]", "", dt$r.customer_review)





tidy_review <- dt %>% 
  unnest_tokens(word,r.customer_review) %>%
  anti_join(stop_words2)



tidy_review %>% 
  count(word) %>% 
  arrange(desc(n)) %>% head(50)






```

```{r word_count}

word_counts <- tidy_review %>%
  count(word) %>%
  filter(n>2500) %>%
  mutate(word2 = fct_reorder(word, n)) %>%  arrange(desc(n))




ggplot(word_counts[1:40,], aes(x=word2, y=n)) + 
  geom_col() +
  coord_flip() +
  ggtitle("Review Word Counts")
```

Since this is a airline customer review data airline\` service and related words are appearing more. Besides words about experienced people wrote about timing and airport service.

```{r}
word_airline_count <- tidy_review %>%
  count(word,r.airline) %>% 
  group_by(r.airline) %>% 
  top_n(20,n) %>% 
  ungroup() %>%
  mutate(word2 = fct_reorder(word, n)) %>% 
  arrange(desc(n))




ggplot(word_airline_count[1:50,], aes(x=word2, y=n, fill=r.airline)) + 
  geom_col(show.legend=FALSE) +
  facet_wrap(~r.airline, scales="free_y") +
  coord_flip() +
  ggtitle("Review Word Counts")

```

This plot shows us which words are most popular according to airline , as we can see for spirit airlines people wrote about time however for other airlines most common words are places which are the hub\`s of respective airlines.

```{r sentiment_review}
sentiment_review <-tidy_review %>%
  inner_join(get_sentiments("loughran")) %>% count(r.overall, sentiment) %>%
  spread(sentiment, n)

pander(sentiment_review)
```

Above table shows us what kind of words has been used for ratings as we expected most negative words belongs to 1 or 2 rating. In our rating column we have NAs as well but with this table we can make assumption what could be people\`s rating.

```{r}
sentimental_score <-tidy_review %>%
  inner_join(get_sentiments("bing")) %>%
  count(r.overall, sentiment) %>%
  spread(sentiment, n) %>% 
  mutate(overall_sentiment = positive - negative,
         Stars = fct_reorder(as.factor(r.overall), overall_sentiment))


ggplot(
  sentimental_score, aes(x=Stars, y=overall_sentiment, fill=as.factor(Stars))
) + 
  geom_col(show.legend=FALSE) +
  coord_flip() +
  labs(
    title = "Overall Sentiment by Stars",
    subtitle = "Reviews for Flight",
    x = "Stars",
    y = "Overall Sentiment"
  )
```

Above plot shows the which rating has positive and negative sentiment score, we can see that from 1 to 5 people wrote negative reviews and they have changed their attitude after 6 star.
